{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Setup File for arXiv Semantic Search App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (2.17.0)\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting psycopg\n",
      "  Downloading psycopg-3.2.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting pgvector\n",
      "  Downloading pgvector-0.3.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.5-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/nima/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (70.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/nima/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (2.9.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (0.20.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /home/nima/.local/lib/python3.10/site-packages (from chromadb) (4.66.4)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (0.12.5)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx>=0.27.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: namex in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/nima/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nima/.local/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy>=1.12.0 (from tensorflow-hub)\n",
      "  Using cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting ml-dtypes (from keras>=3.2.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.25.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /home/nima/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nima/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nima/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/nima/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nima/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading psycopg-3.2.3-py3-none-any.whl (197 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.9/197.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pgvector-0.3.6-py3-none-any.whl (24 kB)\n",
      "Downloading kagglehub-0.3.5-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.7/221.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m897.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=c8892eda77b261be4dc9eeb00f0db36eae0b76013a7dad68cbca69582ad84b3a\n",
      "  Stored in directory: /home/nima/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, websockets, websocket-client, uvloop, uvicorn, python-dotenv, pyproject_hooks, pyasn1, psycopg2-binary, psycopg, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, numpy, mmh3, importlib-resources, humanfriendly, httptools, googleapis-common-protos, deprecated, cachetools, bcrypt, backoff, asgiref, watchfiles, tensorboard, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, pgvector, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, ml-dtypes, kagglehub, coloredlogs, chroma-hnswlib, build, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, keras, google-auth, fastapi, tensorflow, opentelemetry-sdk, opentelemetry-instrumentation-asgi, kubernetes, tf-keras, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, tensorflow-hub, chromadb\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fairlearn 0.11.0 requires scikit-learn>=1.2.1, but you have scikit-learn 1.1.3 which is incompatible.\n",
      "omnixai 1.3.1 requires numpy<1.24,>=1.17, but you have numpy 2.0.2 which is incompatible.\n",
      "torchvision 0.18.1 requires torch==2.3.1, but you have torch 2.5.1 which is incompatible.\n",
      "u8darts 0.30.0 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 deprecated-1.2.15 durationpy-0.9 fastapi-0.115.6 google-auth-2.37.0 googleapis-common-protos-1.66.0 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.4.5 kagglehub-0.3.5 keras-3.7.0 kubernetes-31.0.0 ml-dtypes-0.4.1 mmh3-5.0.1 monotonic-1.6 numpy-2.0.2 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.12 overrides-7.7.0 pgvector-0.3.6 posthog-3.7.4 psycopg-3.2.3 psycopg2-binary-2.9.10 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 requests-oauthlib-2.0.0 rsa-4.9 starlette-0.41.3 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-hub-0.16.1 tf-keras-2.18.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3 websocket-client-1.8.0 websockets-14.1\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install tensorflow tensorflow-hub chromadb psycopg psycopg2-binary pgvector kagglehub streamlit numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/nima/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/nima/.local/lib/python3.10/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/nima/.local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/nima/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/nima/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/nima/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/nima/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/nima/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/nima/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/nima/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nima/miniconda3/envs/Sara/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m983.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add PostgreSQL Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget, you need to setup **pgvector** beforehand for this code to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL setup\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"password\"  # adjust as needed\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5434  # specify your custom port here\n",
    "PG_DBNAME = \"arxivdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Newest arXiv Dataset from Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Nima\\.cache\\kagglehub\\datasets\\Cornell-University\\arxiv\\versions\\210\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either load your desired number of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 documents.\n"
     ]
    }
   ],
   "source": [
    "# Load a small subset of ArXiv metadata for demonstration\n",
    "import json\n",
    "\n",
    "\n",
    "DATA_FILE = f\"{path}/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Let's just read a few records to keep it manageable\n",
    "docs = []\n",
    "max_docs = 200000 # adjust this number as you wish\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_docs:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # Each data item is a dictionary with keys like 'title' and 'abstract'\n",
    "        if 'title' in data and 'abstract' in data:\n",
    "            docs.append({\n",
    "                'id': data['id'],\n",
    "                'title': data['title'],\n",
    "                'abstract': data['abstract']\n",
    "            })\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in the dataset: 2626136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7dc40f999c4a28b405fb8e4fe81446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ArXiv documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2626136 documents.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # Use tqdm.auto if preferred\n",
    "import json\n",
    "\n",
    "DATA_FILE = \"/home/nima/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/210/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "docs = []\n",
    "\n",
    "\n",
    "def get_total_lines(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return sum(buf.count(b'\\n') for buf in iter(lambda: f.read(1024*1024), b''))\n",
    "\n",
    "total_lines = get_total_lines(DATA_FILE)\n",
    "print(f\"Total lines in the dataset: {total_lines}\")\n",
    "\n",
    "# Loading without knowing the total number of lines\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    for line in tqdm(f, desc=\"Loading ArXiv documents\"):\n",
    "        data = json.loads(line)\n",
    "        if 'title' in data and 'abstract' in data:\n",
    "            docs.append({\n",
    "                'id': data['id'],\n",
    "                'title': data['title'],\n",
    "                'abstract': data['abstract']\n",
    "            })\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Embedding Model (Google Universal Sentence Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have CUDA setup you can force TensorFlow to use CPU by uncommenting the related code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nima\\miniconda3\\envs\\420\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Nima\\miniconda3\\envs\\420\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nima\\miniconda3\\envs\\420\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nima\\miniconda3\\envs\\420\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nima\\miniconda3\\envs\\420\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Force TensorFlow to use the CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Load the Universal Sentence Encoder model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# To be embedded\n",
    "texts = [d['title'] + \" \" + d['abstract'] for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB and PGVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection and Batch Processing the Embedded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'arxivdb' already exists.\n",
      "pgvector extension is ensured to be available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac26890f807426098b84a1ae9cbadf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing complete! Data stored in both ChromaDB and PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import psycopg\n",
    "import pgvector.psycopg\n",
    "\n",
    "# ChromaDB setup\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb_arxiv\")\n",
    "try:\n",
    "    chroma_client.delete_collection(\"arxiv_collection\")\n",
    "except:\n",
    "    pass\n",
    "collection = chroma_client.create_collection(\"arxiv_collection\")\n",
    "\n",
    "# Connect to the default 'postgres' database to check for 'arxivdb'\n",
    "with psycopg.connect(\n",
    "    user=PG_USER,\n",
    "    password=PG_PASSWORD,\n",
    "    host=PG_HOST,\n",
    "    port=PG_PORT,\n",
    "    dbname=\"postgres\",  # connect to the default database\n",
    "    autocommit=True\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Check if 'arxivdb' exists\n",
    "        cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (PG_DBNAME,))\n",
    "        exists = cur.fetchone()\n",
    "\n",
    "        # Create 'arxivdb' if it doesn't exist\n",
    "        if not exists:\n",
    "            cur.execute(f\"CREATE DATABASE {PG_DBNAME};\")\n",
    "            print(f\"Database '{PG_DBNAME}' created.\")\n",
    "        else:\n",
    "            print(f\"Database '{PG_DBNAME}' already exists.\")\n",
    "\n",
    "# Connect to the 'arxivdb' database\n",
    "with psycopg.connect(\n",
    "    user=PG_USER,\n",
    "    password=PG_PASSWORD,\n",
    "    host=PG_HOST,\n",
    "    port=PG_PORT,\n",
    "    dbname=PG_DBNAME,\n",
    "    autocommit=True\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Create the pgvector extension if it doesn't exist\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "        print(\"pgvector extension is ensured to be available.\")\n",
    "\n",
    "    # Register the vector type with the connection\n",
    "    pgvector.psycopg.register_vector(conn)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Drop and recreate the 'arxiv_items' table\n",
    "        cur.execute(\"DROP TABLE IF EXISTS arxiv_items;\")\n",
    "        cur.execute(\"CREATE TABLE arxiv_items (id TEXT PRIMARY KEY, title TEXT, abstract TEXT, embedding vector(512));\")\n",
    "\n",
    "        # Process and store embeddings in batches\n",
    "        batch_size = 1000  # Adjust this depending on your system's memory\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            # Prepare batch data\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_docs = docs[i:i+batch_size]\n",
    "            batch_embeddings = embed(batch_texts).numpy()\n",
    "\n",
    "            # Insert into ChromaDB\n",
    "            collection.add(\n",
    "                ids=[doc['id'] for doc in batch_docs],\n",
    "                embeddings=batch_embeddings.tolist(),\n",
    "                metadatas=[{'title': doc['title'], 'abstract': doc['abstract']} for doc in batch_docs],\n",
    "                documents=[doc['abstract'] for doc in batch_docs]\n",
    "            )\n",
    "\n",
    "            # Insert into PostgreSQL\n",
    "            for doc, emb in zip(batch_docs, batch_embeddings):\n",
    "                cur.execute(\n",
    "                    \"INSERT INTO arxiv_items (id, title, abstract, embedding) VALUES (%s, %s, %s, %s);\",\n",
    "                    (doc['id'], doc['title'], doc['abstract'], emb)\n",
    "                )\n",
    "\n",
    "        # Create vector index for cosine similarity in PostgreSQL\n",
    "        cur.execute(\"CREATE INDEX arxiv_items_embedding_idx ON arxiv_items USING hnsw (embedding vector_cosine_ops);\")\n",
    "\n",
    "print(\"Batch processing complete! Data stored in both ChromaDB and PostgreSQL.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing IDs and Loading them (if there was an error with your batch processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import psycopg\n",
    "import pgvector.psycopg\n",
    "\n",
    "# Function to get ChromaDB collection\n",
    "def get_chroma_collection():\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb_arxiv\")\n",
    "    try:\n",
    "        return client.get_collection(\"arxiv_collection\")\n",
    "    except:\n",
    "        # If collection does not exist, create it\n",
    "        return client.create_collection(\"arxiv_collection\")\n",
    "\n",
    "# Function to connect to PostgreSQL\n",
    "def get_pg_connection():\n",
    "    conn = psycopg.connect(user=PG_USER, password=PG_PASSWORD, host=PG_HOST, port=PG_PORT,dbname=PG_DBNAME, autocommit=True)\n",
    "    pgvector.psycopg.register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "# Fetch IDs from databases\n",
    "def get_existing_ids():\n",
    "    # Fetch IDs from ChromaDB\n",
    "    collection = get_chroma_collection()\n",
    "    \n",
    "    # Retrieve all data and extract IDs\n",
    "    chroma_data = collection.get()\n",
    "    existing_chromadb_ids = set(chroma_data[\"ids\"])\n",
    "\n",
    "    # Fetch IDs from PostgreSQL\n",
    "    conn = get_pg_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id FROM arxiv_items;\")\n",
    "    existing_postgres_ids = {row[0] for row in cur.fetchall()}\n",
    "    conn.close()\n",
    "\n",
    "    return existing_chromadb_ids, existing_postgres_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IDs in ChromaDB: 200\n",
      "Number of IDs in PostgreSQL: 200\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "existing_chromadb_ids, existing_postgres_ids = get_existing_ids()\n",
    "print(f\"Number of IDs in ChromaDB: {len(existing_chromadb_ids)}\")\n",
    "print(f\"Number of IDs in PostgreSQL: {len(existing_postgres_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify Missing IDs\n",
    "dataset_ids = {doc['id'] for doc in docs}  # Assuming `docs` contains your dataset\n",
    "missing_ids = dataset_ids - existing_chromadb_ids - existing_postgres_ids\n",
    "len(missing_ids)  # Number of missing IDs\n",
    "\n",
    "# Step 2: Filter the Dataset for Missing IDs\n",
    "missing_docs = [doc for doc in docs if doc['id'] in missing_ids]\n",
    "missing_texts = [doc['title'] + \" \" + doc['abstract'] for doc in missing_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Results of Missing to JSON (Checkpoint in case of crash due to low RAM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing documents saved to 'missing_docs.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save missing_docs to a JSON file\n",
    "with open(\"missing_docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(missing_docs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Missing documents saved to 'missing_docs.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 96120 missing documents.\n"
     ]
    }
   ],
   "source": [
    "# Load missing_docs from the JSON file\n",
    "with open(\"missing_docs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    missing_docs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(missing_docs)} missing documents.\")\n",
    "missing_texts = [doc['title'] + \" \" + doc['abstract'] for doc in missing_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Missing Rows (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import psycopg\n",
    "import pgvector.psycopg\n",
    "\n",
    "# Function to get ChromaDB collection\n",
    "def get_chroma_collection():\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb_arxiv\")\n",
    "    try:\n",
    "        return client.get_collection(\"arxiv_collection\")\n",
    "    except:\n",
    "        # If collection does not exist, create it\n",
    "        return client.create_collection(\"arxiv_collection\")\n",
    "\n",
    "# Function to connect to PostgreSQL\n",
    "def get_pg_connection():\n",
    "    conn = psycopg.connect(user=PG_USER, password=PG_PASSWORD, host=PG_HOST, dbname=PG_DBNAME, autocommit=True)\n",
    "    pgvector.psycopg.register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "# Insert missing items into ChromaDB and PostgreSQL\n",
    "def insert_missing_items(missing_docs, embed, batch_size=500):\n",
    "    # Connect to databases\n",
    "    collection = get_chroma_collection()\n",
    "    conn = get_pg_connection()\n",
    "\n",
    "    # Process and insert in batches\n",
    "    for i in tqdm(range(0, len(missing_texts), batch_size), desc=\"Inserting missing items\"):\n",
    "        batch_texts = missing_texts[i:i + batch_size]\n",
    "        batch_docs = missing_docs[i:i + batch_size]\n",
    "        batch_embeddings = embed(batch_texts).numpy()\n",
    "\n",
    "        # Insert into ChromaDB\n",
    "        collection.add(\n",
    "            ids=[doc['id'] for doc in batch_docs],\n",
    "            embeddings=batch_embeddings.tolist(),\n",
    "            metadatas=[{'title': doc['title'], 'abstract': doc['abstract']} for doc in batch_docs],\n",
    "            documents=[doc['abstract'] for doc in batch_docs]\n",
    "        )\n",
    "\n",
    "        # Insert into PostgreSQL\n",
    "        cur = conn.cursor()\n",
    "        for doc, emb in zip(batch_docs, batch_embeddings):\n",
    "            cur.execute(\n",
    "                \"INSERT INTO arxiv_items (id, title, abstract, embedding) VALUES (%s, %s, %s, %s);\",\n",
    "                (doc['id'], doc['title'], doc['abstract'], emb)\n",
    "            )\n",
    "\n",
    "    # Close PostgreSQL connection\n",
    "    conn.close()\n",
    "    print(\"Insertion of missing items completed.\")\n",
    "\n",
    "insert_missing_items(missing_docs, embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PGVector Indexes (HNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create HNSW index: operator class \"vector_hnsw_ops\" does not exist for access method \"hnsw\"\n",
      "CONTEXT:  SQL statement \"CREATE INDEX embedding_hnsw_idx\n",
      "                    ON arxiv_items USING hnsw (embedding vector_hnsw_ops)\"\n",
      "PL/pgSQL function inline_code_block line 4 at SQL statement\n"
     ]
    }
   ],
   "source": [
    "def create_hnsw_index(conn):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"\"\"\n",
    "            DO $$\n",
    "            BEGIN\n",
    "                IF NOT EXISTS (SELECT 1 FROM pg_indexes WHERE tablename = 'arxiv_items' AND indexname = 'embedding_hnsw_idx') THEN\n",
    "                    CREATE INDEX embedding_hnsw_idx\n",
    "                    ON arxiv_items USING hnsw (embedding vector_hnsw_ops);\n",
    "                END IF;\n",
    "            END $$;\n",
    "        \"\"\")\n",
    "        cur.close()\n",
    "        print(\"HNSW index created successfully (or already exists).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create HNSW index: {e}\")\n",
    "\n",
    "\n",
    "pg_conn = get_pg_connection()\n",
    "create_hnsw_index(pg_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Streamlit App: app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import psycopg\n",
    "import chromadb\n",
    "import tensorflow_hub as hub\n",
    "import pgvector.psycopg\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "\n",
    "# Load USE once and cache\n",
    "@st.cache_resource\n",
    "def load_use_model():\n",
    "    return hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "@st.cache_resource\n",
    "def get_chroma_collection():\n",
    "    client = chromadb.PersistentClient(path=\"./chromadb_arxiv\")\n",
    "    return client.get_collection(\"arxiv_collection\")\n",
    "\n",
    "@st.cache_resource\n",
    "def get_pg_connection():\n",
    "    PG_USER = \"postgres\"\n",
    "    PG_PASSWORD = \"passkey\"  # adjust accordingly\n",
    "    PG_HOST = \"localhost\"\n",
    "    PG_DBNAME = \"arxivdb\"\n",
    "    conn = psycopg.connect(user=PG_USER, password=PG_PASSWORD, host=PG_HOST, dbname=PG_DBNAME, autocommit=True)\n",
    "    pgvector.psycopg.register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "def search_chroma(query_embedding, top_k=5, coll=None):\n",
    "    start = time.monotonic()\n",
    "    results = coll.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "    latency = max(0, time.monotonic() - start)\n",
    "    hits = [\n",
    "        {\n",
    "            'id': results['ids'][0][i],\n",
    "            'abstract': results['documents'][0][i],\n",
    "            'title': results['metadatas'][0][i]['title']\n",
    "        }\n",
    "        for i in range(len(results['ids'][0]))\n",
    "    ]\n",
    "    return hits, latency\n",
    "\n",
    "def search_pgvector(query_embedding, top_k=5, conn=None):\n",
    "    cur = conn.cursor()\n",
    "    start = time.monotonic()\n",
    "    cur.execute(\"SELECT id, title, abstract FROM arxiv_items ORDER BY embedding <=> %s LIMIT %s;\", (query_embedding, top_k))\n",
    "    rows = cur.fetchall()\n",
    "    latency = max(0, time.monotonic() - start)\n",
    "    hits = [{'id': r[0], 'title': r[1], 'abstract': r[2]} for r in rows]\n",
    "    return hits, latency\n",
    "\n",
    "\n",
    "# Establish connections as soon as the app starts\n",
    "use_model = load_use_model()\n",
    "chroma_coll = get_chroma_collection()\n",
    "pg_conn = get_pg_connection()\n",
    "\n",
    "st.title(\"ArXiv Semantic Search: Performance Comparison\")\n",
    "\n",
    "# Sidebar options\n",
    "db_option = st.sidebar.selectbox(\"Select vector DB:\", [\"ChromaDB\", \"PGVector\"])\n",
    "query = st.text_input(\"Enter your query:\", \"\")\n",
    "top_k = st.sidebar.slider(\"Number of results (k):\", min_value=1, max_value=20, value=5)\n",
    "\n",
    "# Maintain stateful metrics\n",
    "if 'chroma_query_times' not in st.session_state:\n",
    "    st.session_state.chroma_query_times = []\n",
    "if 'pgvector_query_times' not in st.session_state:\n",
    "    st.session_state.pgvector_query_times = []\n",
    "\n",
    "# Placeholder for recall metrics (if labels are available)\n",
    "if 'recall_chroma' not in st.session_state:\n",
    "    st.session_state.recall_chroma = []\n",
    "if 'recall_pgvector' not in st.session_state:\n",
    "    st.session_state.recall_pgvector = []\n",
    "\n",
    "# Search Section\n",
    "st.header(\"Single Query Search\")\n",
    "\n",
    "if st.button(\"Search\"):\n",
    "    if query.strip():\n",
    "        q_emb = use_model([query]).numpy()[0]\n",
    "\n",
    "        if db_option == \"ChromaDB\":\n",
    "            results, latency = search_chroma(q_emb, top_k=top_k, coll=chroma_coll)\n",
    "            st.session_state.chroma_query_times.append(latency)\n",
    "            # Placeholder for recall calculation\n",
    "            # If labels are available, compute recall here\n",
    "            # For example:\n",
    "            # recall = compute_recall(true_relevant_docs, results)\n",
    "            # st.session_state.recall_chroma.append(recall)\n",
    "        else:\n",
    "            results, latency = search_pgvector(q_emb, top_k=top_k, conn=pg_conn)\n",
    "            st.session_state.pgvector_query_times.append(latency)\n",
    "            # Placeholder for recall calculation\n",
    "            # If labels are available, compute recall here\n",
    "            # recall = compute_recall(true_relevant_docs, results)\n",
    "            # st.session_state.recall_pgvector.append(recall)\n",
    "\n",
    "        st.write(f\"**Results from {db_option}** (Query took {latency:.4f} seconds):\")\n",
    "        for res in results:\n",
    "            st.write(f\"**Title:** {res['title']}\")\n",
    "            st.write(f\"**Abstract:** {res['abstract']}\")\n",
    "            st.write(\"---\")\n",
    "\n",
    "        # Show average query times\n",
    "        if st.session_state.chroma_query_times:\n",
    "            avg_chroma = np.mean(st.session_state.chroma_query_times)\n",
    "        else:\n",
    "            avg_chroma = None\n",
    "\n",
    "        if st.session_state.pgvector_query_times:\n",
    "            avg_pg = np.mean(st.session_state.pgvector_query_times)\n",
    "        else:\n",
    "            avg_pg = None\n",
    "\n",
    "        st.write(\"## Performance Metrics\")\n",
    "        st.write(f\"ChromaDB Average Query Time: {avg_chroma:.4f}s\" if avg_chroma is not None else \"No ChromaDB queries yet.\")\n",
    "        st.write(f\"PGVector Average Query Time: {avg_pg:.4f}s\" if avg_pg is not None else \"No PGVector queries yet.\")\n",
    "\n",
    "        # Display recall if available\n",
    "        if st.session_state.recall_chroma or st.session_state.recall_pgvector:\n",
    "            st.write(\"## Recall Metrics\")\n",
    "            if st.session_state.recall_chroma:\n",
    "                avg_recall_chroma = np.mean(st.session_state.recall_chroma)\n",
    "                st.write(f\"ChromaDB Average Recall: {avg_recall_chroma:.4f}\")\n",
    "            else:\n",
    "                st.write(\"No ChromaDB recall metrics yet.\")\n",
    "            if st.session_state.recall_pgvector:\n",
    "                avg_recall_pgvector = np.mean(st.session_state.recall_pgvector)\n",
    "                st.write(f\"PGVector Average Recall: {avg_recall_pgvector:.4f}\")\n",
    "            else:\n",
    "                st.write(\"No PGVector recall metrics yet.\")\n",
    "    else:\n",
    "        st.warning(\"Please enter a query.\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Benchmarking Section\n",
    "st.header(\"Benchmarking\")\n",
    "\n",
    "benchmark_mode = st.radio(\"Select Benchmark Mode:\", [\"Manual Queries\", \"Upload Query File\"])\n",
    "\n",
    "queries = []\n",
    "\n",
    "if benchmark_mode == \"Manual Queries\":\n",
    "    st.subheader(\"Enter Multiple Queries\")\n",
    "    num_queries = st.number_input(\"Number of queries to run:\", min_value=1, max_value=1000, value=10)\n",
    "    for i in range(int(num_queries)):\n",
    "        q = st.text_input(f\"Query {i+1}:\", key=f\"manual_query_{i}\")\n",
    "        if q:\n",
    "            queries.append(q)\n",
    "elif benchmark_mode == \"Upload Query File\":\n",
    "    st.subheader(\"Upload a File with Queries\")\n",
    "    uploaded_file = st.file_uploader(\"Choose a text file with one query per line\", type=[\"txt\"])\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
    "            queries = [line.strip() for line in stringio if line.strip()]\n",
    "            st.write(f\"Loaded {len(queries)} queries.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading file: {e}\")\n",
    "\n",
    "if queries:\n",
    "    run_benchmark = st.button(\"Run Benchmark\")\n",
    "    \n",
    "    if run_benchmark:\n",
    "        # Initialize lists to store latencies and recall (if possible)\n",
    "        chroma_latencies = []\n",
    "        pgvector_latencies = []\n",
    "        # Placeholder for recall metrics\n",
    "        chroma_recalls = []\n",
    "        pgvector_recalls = []\n",
    "\n",
    "        progress_bar = st.progress(0)\n",
    "        status_text = st.empty()\n",
    "\n",
    "        for idx, q in enumerate(queries):\n",
    "            status_text.text(f\"Processing query {idx+1}/{len(queries)}: {q[:50]}...\")\n",
    "\n",
    "            q_emb = use_model([q]).numpy()[0]\n",
    "\n",
    "            # ChromaDB search\n",
    "            results_chroma, chroma_latency = search_chroma(q_emb, top_k=top_k, coll=chroma_coll)\n",
    "            chroma_latencies.append(chroma_latency)\n",
    "\n",
    "            # PGVector search\n",
    "            results_pgvector, pg_latency = search_pgvector(q_emb, top_k=top_k, conn=pg_conn)\n",
    "            pgvector_latencies.append(pg_latency)\n",
    "\n",
    "            progress_bar.progress((idx + 1) / len(queries))\n",
    "\n",
    "        progress_bar.empty()\n",
    "        status_text.text(\"Benchmark completed.\")\n",
    "\n",
    "        # Convert latencies to DataFrame\n",
    "        df_benchmark = pd.DataFrame({\n",
    "            'ChromaDB Latency (s)': chroma_latencies,\n",
    "            'PGVector Latency (s)': pgvector_latencies\n",
    "        })\n",
    "\n",
    "        st.subheader(\"Benchmark Results\")\n",
    "\n",
    "        st.write(df_benchmark.describe())\n",
    "\n",
    "        # Plotting\n",
    "        st.subheader(\"Latency Distribution\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.kdeplot(df_benchmark['ChromaDB Latency (s)'], label='ChromaDB', shade=True, ax=ax)\n",
    "        sns.kdeplot(df_benchmark['PGVector Latency (s)'], label='PGVector', shade=True, ax=ax)\n",
    "        ax.set_xlabel(\"Latency (seconds)\")\n",
    "        ax.set_title(\"Latency Distribution for ChromaDB and PGVector\")\n",
    "        ax.legend(title='Database')\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        st.subheader(\"Latency Comparison\")\n",
    "\n",
    "        df_melted = df_benchmark.melt(var_name='Database', value_name='Latency (s)')\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(x='Database', y='Latency (s)', data=df_melted, ax=ax2)\n",
    "        ax2.set_title(\"Latency Comparison between ChromaDB and PGVector\")\n",
    "        ax2.legend().remove()  # Boxplot has inherent labels\n",
    "        st.pyplot(fig2)\n",
    "\n",
    "        st.subheader(\"Latency Over Queries\")\n",
    "\n",
    "        fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "        ax3.plot(range(1, len(chroma_latencies)+1), chroma_latencies, label='ChromaDB', marker='o')\n",
    "        ax3.plot(range(1, len(pgvector_latencies)+1), pgvector_latencies, label='PGVector', marker='x')\n",
    "        ax3.set_xlabel(\"Query Number\")\n",
    "        ax3.set_ylabel(\"Latency (seconds)\")\n",
    "        ax3.set_title(\"Latency Over Queries\")\n",
    "        ax3.legend(title='Database')\n",
    "        st.pyplot(fig3)\n",
    "\n",
    "        # Optionally, download the benchmark results\n",
    "        csv = df_benchmark.to_csv(index=False).encode('utf-8')\n",
    "        st.download_button(\n",
    "            label=\"Download Benchmark Results as CSV\",\n",
    "            data=csv,\n",
    "            file_name='benchmark_results.csv',\n",
    "            mime='text/csv',\n",
    "        )\n",
    "\n",
    "        # If recall metrics are available, display them\n",
    "        if chroma_recalls or pgvector_recalls:\n",
    "            df_recall = pd.DataFrame({\n",
    "                'ChromaDB Recall': chroma_recalls,\n",
    "                'PGVector Recall': pgvector_recalls\n",
    "            })\n",
    "\n",
    "            st.subheader(\"Recall Metrics\")\n",
    "\n",
    "            st.write(df_recall.describe())\n",
    "\n",
    "            # Plot Recall Distribution\n",
    "            fig_recall, ax_recall = plt.subplots(figsize=(10, 6))\n",
    "            sns.kdeplot(df_recall['ChromaDB Recall'], label='ChromaDB', shade=True, ax=ax_recall)\n",
    "            sns.kdeplot(df_recall['PGVector Recall'], label='PGVector', shade=True, ax=ax_recall)\n",
    "            ax_recall.set_xlabel(\"Recall\")\n",
    "            ax_recall.set_title(\"Recall Distribution for ChromaDB and PGVector\")\n",
    "            ax_recall.legend(title='Database')\n",
    "            st.pyplot(fig_recall)\n",
    "\n",
    "            # Plot Recall Comparison\n",
    "            df_recall_melted = df_recall.melt(var_name='Database', value_name='Recall')\n",
    "            fig_recall_box, ax_recall_box = plt.subplots(figsize=(10, 6))\n",
    "            sns.boxplot(x='Database', y='Recall', data=df_recall_melted, ax=ax_recall_box)\n",
    "            ax_recall_box.set_title(\"Recall Comparison between ChromaDB and PGVector\")\n",
    "            ax_recall_box.legend().remove()  # Boxplot has inherent labels\n",
    "            st.pyplot(fig_recall_box)\n",
    "\n",
    "            # Plot Recall Over Queries\n",
    "            fig_recall_over, ax_recall_over = plt.subplots(figsize=(10, 6))\n",
    "            ax_recall_over.plot(range(1, len(chroma_recalls)+1), chroma_recalls, label='ChromaDB', marker='o')\n",
    "            ax_recall_over.plot(range(1, len(pgvector_recalls)+1), pgvector_recalls, label='PGVector', marker='x')\n",
    "            ax_recall_over.set_xlabel(\"Query Number\")\n",
    "            ax_recall_over.set_ylabel(\"Recall\")\n",
    "            ax_recall_over.set_title(\"Recall Over Queries\")\n",
    "            ax_recall_over.legend(title='Database')\n",
    "            st.pyplot(fig_recall_over)\n",
    "\n",
    "    # Optionally, if recall is computed during benchmark, add it here\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Performance Metrics Visualization (optional)\n",
    "st.header(\"Performance Metrics Visualization\")\n",
    "\n",
    "if st.session_state.chroma_query_times or st.session_state.pgvector_query_times:\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'ChromaDB': st.session_state.chroma_query_times,\n",
    "        'PGVector': st.session_state.pgvector_query_times\n",
    "    })\n",
    "\n",
    "    st.subheader(\"Historical Query Latencies\")\n",
    "\n",
    "    fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_metrics, ax=ax4)\n",
    "    ax4.set_xlabel(\"Query Number\")\n",
    "    ax4.set_ylabel(\"Latency (seconds)\")\n",
    "    ax4.set_title(\"Historical Query Latencies\")\n",
    "    ax4.legend(title='Database')\n",
    "    st.pyplot(fig4)\n",
    "\n",
    "    st.subheader(\"Latency Distribution (Historical)\")\n",
    "\n",
    "    fig5, ax5 = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(df_metrics['ChromaDB'], color='blue', label='ChromaDB', kde=True, stat=\"density\", linewidth=0)\n",
    "    sns.histplot(df_metrics['PGVector'], color='orange', label='PGVector', kde=True, stat=\"density\", linewidth=0, alpha=0.6)\n",
    "    ax5.legend(title='Database')\n",
    "    ax5.set_xlabel(\"Latency (seconds)\")\n",
    "    ax5.set_title(\"Latency Distribution (Historical Queries)\")\n",
    "    st.pyplot(fig5)\n",
    "else:\n",
    "    st.write(\"No historical query data to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT:\n",
    "**In terminal, go to the directory where app.py is and run:**\n",
    "`Streamlit run app.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "420",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
